# -*- coding: utf-8 -*-
"""Classification algorithms.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PpwZtkW6gR6pU7GvK7KtwmtRJOLG1ex_

# Classification algorithms
"""

import numpy as np

"""When developing algorithms it is often desirable to have datasets for testing purposes. When there is no access to a suitable dataset, sometimes it is convenient to generate synthetic data. In this tutorial, we will  
1. create synthetic datasets consisting of two classes of objects;
2. develop the k-NN algorithm;
3. evaluate the algorithm's ability to separate the two classes of objects.

## Excercise 1. Create synthetic dataset

We would like to generate training and validation datasets for binary classification. For visualisation purposes, we will assume that our objects have 2 numeric features. We would like to generate 2 "cloulds" of points on the plane corresponding to the positive and negative objects respectively. To do this, one can generate random points from a [multivariate normal distribution](https://en.wikipedia.org/wiki/Multivariate_normal_distribution) (function $\texttt{np.random.multivariate_normal}$). For example, $\texttt{np.random.multivariate_normal([a,b], [[1,0],[0,1]], N)}$ will generate a set on N points scattered around the *mean* point $(a,b)$.

1. Create two sets of $N=10$ points. The first set scattered around the point $(0,0)$ and the second scattered around the point $(2,2)$. These sets of points will correspond to the positive and the negative class respectively.
"""

N = 10

# Generate synthetic datasets
mean_positive = [0, 0]
mean_negative = [2, 2]
covariance = [[1, 0], [0, 1]]

positive_points = np.random.multivariate_normal(mean_positive, covariance, N)
negative_points = np.random.multivariate_normal(mean_negative, covariance, N)

"""2. Plot the generated sets of points. Use different colours or markers for different classes."""

import matplotlib.pyplot as plt

# Plot the generated sets of points
plt.scatter(positive_points[:, 0], positive_points[:, 1], color='blue', label='Positive')
plt.scatter(negative_points[:, 0], negative_points[:, 1], color='red', label='Negative')
plt.legend()
plt.show()

"""3. Split each of the sets into equal train and validation portions. As a result you should have four sets:
- positive object in the train dataset;
- positive object in the validation dataset;
- negataive object in the train dataset;
- negataive object in the validation dataset;

To confirm that the sets have equal numbers of objects, print the number of elements in each set.
"""

# Split into train and validation sets
positive_train = positive_points[:N//2]
positive_val = positive_points[N//2:]
negative_train = negative_points[:N//2]
negative_val = negative_points[N//2:]

print(f"Positive train set size: {len(positive_train)}")
print(f"Positive validation set size: {len(positive_val)}")
print(f"Negative train set size: {len(negative_train)}")
print(f"Negative validation set size: {len(negative_val)}")

"""4. Add an extra freature (representing the class label: +1 for the positive class, -1 for the negative class) to the train and validation instances. As a result you will have two datasets, each consisting of tuples (label, instance)."""

# Add class labels
train_data = [(1, point) for point in positive_train] + [(-1, point) for point in negative_train]
val_data = [(1, point) for point in positive_val] + [(-1, point) for point in negative_val]

"""## Excercise 2. Develop the k-NN algorithm

Implement k-NN prediction function that uses the training dataset from the previous exercise. Use cosine similarity as a "measure of distance". The larger the similarity between two objects, the closer the objects are to each other.

1. Create the cosine similarity function that will be used in the k-NN prediction function to find the neighbours. The function should take two vectors as input and output the cosine similarity between the vectors.
"""

# Cosine similarity function
def cosine_similarity(vec1, vec2):
    dot_product = np.dot(vec1, vec2)
    norm_vec1 = np.linalg.norm(vec1)
    norm_vec2 = np.linalg.norm(vec2)
    return dot_product / (norm_vec1 * norm_vec2)

"""2. Implement a function that predicts the class of a validation instance using the k-NN algorithm. The function should take a validation instance and the parameter $k$ as input, and output predicted class of the validation instance (+1 or -1)."""

# k-NN prediction function
def k_nn_predict(validation_instance, k):
    similarities = [(cosine_similarity(validation_instance, instance), label) for label, instance in train_data]
    similarities.sort(reverse=True, key=lambda x: x[0])
    top_k = similarities[:k]
    prediction = sum(label for _, label in top_k)
    return 1 if prediction > 0 else -1

"""## Excercise 3. Evaluate the algorithm

1. Implement $\texttt{kNNaccuracy}$ function that takes the parameter $k$ and the validation dataset as input and output the accuracy of the k-NN algorithm on the validation dataset. Use the function to compute the accuracy of prediciton of the k-NN classifier on the validation dataset, when $k = 5$.
"""

# kNNaccuracy function
def kNNaccuracy(k, validation_dataset):
    correct_predictions = 0
    for label, instance in validation_dataset:
        prediction = k_nn_predict(instance, k)
        if prediction == label:
            correct_predictions += 1
    return correct_predictions / len(validation_dataset)

# Evaluate k-NN accuracy for k=5
accuracy_k_5 = kNNaccuracy(5, val_data)
print(f"Accuracy for k=5: {accuracy_k_5}")

"""2. Generate new datasets with $N=100$. Compute accuracies of k-NN for all odd $k$ from 1 to 99. Plot k-NN accuracy versus $k$. What is the best value of $k$ for the validation dataset?"""

# Generate new datasets with N=100 and evaluate for odd k from 1 to 99
N = 100
positive_points = np.random.multivariate_normal(mean_positive, covariance, N)
negative_points = np.random.multivariate_normal(mean_negative, covariance, N)
positive_train = positive_points[:N//2]
positive_val = positive_points[N//2:]
negative_train = negative_points[:N//2]
negative_val = negative_points[N//2:]
train_data = [(1, point) for point in positive_train] + [(-1, point) for point in negative_train]
val_data = [(1, point) for point in positive_val] + [(-1, point) for point in negative_val]

accuracies = []
for k in range(1, 100, 2):
    accuracy = kNNaccuracy(k, val_data)
    accuracies.append(accuracy)

plt.plot(range(1, 100, 2), accuracies)
plt.xlabel('k')
plt.ylabel('Accuracy')
plt.title('k-NN Accuracy vs k')
plt.show()

"""3. Conduct further experiments:
    * change the value of $k$
    * increase the number of instances $N$ (make sure that $N$ is even)
    * separate or bring together the two classes by adjusting the means of the two Gaussians.

How does the accuracy vary in each case?
"""

# Conduct further experiments by changing k, N, and means

# Experiment 1: Change the value of k
print("Experiment 1: Change the value of k")
for k in [1, 3, 5, 7, 9]:
    accuracy = kNNaccuracy(k, val_data)
    print(f"Accuracy for k={k}: {accuracy}")

# Experiment 2: Increase the number of instances N
print("\nExperiment 2: Increase the number of instances N")
for N in [50, 100, 200]:
    positive_points = np.random.multivariate_normal(mean_positive, covariance, N)
    negative_points = np.random.multivariate_normal(mean_negative, covariance, N)
    positive_train = positive_points[:N//2]
    positive_val = positive_points[N//2:]
    negative_train = negative_points[:N//2]
    negative_val = negative_points[N//2:]
    train_data = [(1, point) for point in positive_train] + [(-1, point) for point in negative_train]
    val_data = [(1, point) for point in positive_val] + [(-1, point) for point in negative_val]
    accuracy = kNNaccuracy(5, val_data)
    print(f"Accuracy for N={N}: {accuracy}")

# Experiment 3: Adjust the means of the two Gaussians
print("\nExperiment 3: Adjust the means of the two Gaussians")
for mean_diff in [1, 2, 3]:
    mean_positive = [0, 0]
    mean_negative = [mean_diff, mean_diff]
    positive_points = np.random.multivariate_normal(mean_positive, covariance, 100)
    negative_points = np.random.multivariate_normal(mean_negative, covariance, 100)
    positive_train = positive_points[:50]
    positive_val = positive_points[50:]
    negative_train = negative_points[:50]
    negative_val = negative_points[50:]
    train_data = [(1, point) for point in positive_train] + [(-1, point) for point in negative_train]
    val_data = [(1, point) for point in positive_val] + [(-1, point) for point in negative_val]
    accuracy = kNNaccuracy(5, val_data)
    print(f"Accuracy for mean difference={mean_diff}: {accuracy}")

